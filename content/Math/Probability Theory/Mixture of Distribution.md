---
title: Mixture of Distribution
thumbnail: ''
draft: false
tags:
- statistics
- distribution
- latent-variable
- EM-algorithm
created: 2024-09-24
---

# 혼합 분포

 > 
 > 특정 분포를 만드는데 있어, 여러개의 확률 분포를 사용하는 것.

* 상황을 하나 생각해보자.
* 1에서 6까지의 숫자가 나열되어 있는 분포를 생각해보자.
* 해당 숫자들은 각기 다른 빈도를 갖고 있을 것이다.
* 그리고 주사위 하나를 생각해보자.
* 위 상황에서, 주사위 하나를 뽑았을 때, 특정 x(1~6)이 나올 확률분포를 구해보자.

$$
P(x)=\sum P(c=i)P(x|c=i)
$$

* 여기서 $P(c=i)$는 [Categorical Distribution](Categorical%20Distribution.md)에서의 단일 시행을 의미한다.
* $P(x|c=i)$는 $c=i$일 때, $x$가 나올 확률을 의미한다.

# 경험 분포에서의 예시

* 우리는 사실 혼합분포를 본적이 있다.
* [Empirical Distribution](Empirical%20Distribution.md)을 생각해보자.

$$
\\hat{p}(x) = \frac{1}{N} \sum\_{i=1}^{N} \delta(x - x_i)
$$

* 경험 분포는 샘플 마다 하나의 디랙 성분이 있는 혼합 분포이다.

# Latent Variable

 > 
 > 직접 측정할 수 없는 확률 변수

* 예를 들어, 사람들의 시험 성적에 영향을 미치는 요인으로 “지능”이라는 잠재 변수를 생각해보자.
* “지능”은 직접적으로 측정할 수 없지만, 시험 성적이나 공부 시간 등과 같은 관찰 가능한 데이터로부터 간접적으로 추론할 수 있다.
* 이러한 잠재 변수는 실제 내가 바라보는 사건 $x$와 연관될 수도 있다.
* 이런 경우 잠재 변수를 $c$라 했을 때, $P(x, c)=P(x|c)P(c)$로 표현할 수 있다.

# Gaussian Mixture Model

대표적인 혼합 모델 중 하나로, 잠재 변수를 활용하여 데이터를 설명하는 강력한 모델.

* 가우시안 혼합 모형(GMM)은 잠재 변수를 사용한 대표적인 모델이다.
* GMM은 각 데이터가 여러 개의 가우시안 분포 중 하나에서 생성된다고 가정한다.
* 즉, 각 데이터가 어떤 분포에서 생성되었는지 직접 알 수 없으므로, 잠재 변수를 도입하여 이를 설명한다.
* EM 알고리즘(Expectation-Maximization)을 통해 잠재 변수를 추정하고, 그에 따라 데이터를 가장 잘 설명하는 가우시안 분포의 파라미터(평균 $\mu$와 공분산 $\Sigma$)를 학습한다.
* GMM에서 $P(x|c=i)$는 성분 i에서의 가우시안 분포를 나타내며, 각각의 성분은 평균과 공분산 ($\mu_i$, $\Sigma_i$)으로 파라미터화된다.
* 성분 간의 공분산이 같거나 다를 수 있으며, 이를 통해 모델에 제약을 줄 수도 있다.

## Prior Probability

* 평균과 공분산 외에, 가우스 혼합 모델의 매개변수들은 각 성분 $i$에 대해 사전확률을 부여한다. $\alpha_i = P(c=i)$
* 여기서 사전이란 **이 확률이 $x$를 관측하기 이전의** $c$에 대한 모형의 확신도를 나타낸다.

## Posterior Probability

* 사후 확률  $P(c|x)$ 는 주어진 데이터 $x$가 특정 성분 $c$에 속할 확률이다.
* 이는 데이터를 관찰한 이후 모델이 특정 성분에 대해 가지는 확신도를 나타낸다.
* 이건 최종적으로 GMM에서 사용할 분포를 결정짓는 EM 알고리즘에서 가능도를 계산하기 위해 사용한다. 그냥 일단 알아만 두자.
* GMM의 중요한 속성 중 하나는 충분히 많은 가우시안 성분을 사용하면 임의의 매끄러운 확률 밀도 함수를 매우 정확하게 근사할 수 있다는 점이다.
* 즉, 복잡한 분포라도 여러 가우시안 분포의 혼합으로 거의 완벽하게 표현할 수 있다.
* 가우시안 성분의 수를 늘리고 조정하면, 실제 데이터 분포와 가우시안 혼합 모형이 표현하는 분포 간의 차이를 최소화할 수 있다.
* 이러한 특성 덕분에 GMM은 \*\*보편적 근사기(Universal Approximator)\*\*로 불린다.

# 정리

* **혼합 분포**는 여러 개의 개별 확률 분포를 결합하여 하나의 복잡한 분포를 만드는 방법이다.
* \*\*Latent variable(잠재 변수)\*\*은 직접 관찰할 수 없지만 데이터 생성에 영향을 미치는 숨겨진 확률 변수를 의미한다.
* 이는 특정 데이터가 어떤 분포에서 생성되었는지 모르는 상황을 반영하는데, 예를 들어 우리가 관찰하는 데이터가 어떤 잠재적인 메커니즘에 의해 나왔는지 알 수 없는 경우이다.
* 대부분의 실제 데이터는 이러한 잠재 변수를 포함한 복잡한 구조를 가지고 있기 때문에, **관찰 가능한 데이터만으로는 해당 데이터가 어떤 분포에서 생성되었는지 알 수 없다.**
* 하지만 우리는 잠재 변수를 가정하고, 이를 통해 데이터를 설명할 수 있는 **모델**을 만들어야 한다.
* 직관적인 접근법은, 잠재 변수가 있다고 가정한 후 **모델이 데이터에 가장 적합하게** 작동하도록 학습시키는 것이다.
* **EM 알고리즘**은 이 문제를 해결하는 대표적인 방법으로, 잠재 변수를 고려한 상태에서 데이터가 가장 잘 설명되도록 **가능도(우도, [Likelihood](Likelihood.md))를 최대로 하는 모델 파라미터**를 찾는다.
* EM 알고리즘의 결과로 각 데이터 포인트가 특정 잠재 변수에 속할 확률을 추정하며, 이 잠재 변수가 특정 분포를 나타낸다고 가정한다.
* \*\*가우시안 혼합 모형(GMM)\*\*은 데이터를 여러 개의 가우시안 분포의 혼합으로 설명하는 모델로, EM 알고리즘을 사용하여 각 데이터가 어느 가우시안 분포에서 나왔는지 확률적으로 추정한다.
