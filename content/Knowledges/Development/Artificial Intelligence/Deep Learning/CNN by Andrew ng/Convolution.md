---
title: Convolution
thumbnail: ''
draft: false
tags:
- convolution
- computer-vision
- deep-learning
- edge-detection
created: 2023-10-04
---

# What is convolution

* 하트시그널을 봤다.
* 그 무엇보다도 치열한(..) 구애 행위와 두뇌게임을 봤다.
* 내가 누군가한테 구애 행위를 하는 것은 구애 받는 사람의 상태에 따라 YES, 혹은 NO가 나올 거다.
* 이렇게 우리는 항상 누군가와 상호작용을 하고, 그 결과에 관심이 있다.
* 여기서 구애하는 사람이 하는 행위를 Signal
* 구애 받는 사람의 상태를 System
* 그리고 이 두가지의 상호작용을 통한 결과를 도출하는 것을 Convolution 이라 한다.

## ***EXAMLPLE***

* 난로가 있다.
  ![](Pasted%20image%2020231004141458.png)

* 이 난로는 처음에 많이 따뜻했다가 점점 열기가 식는다. 이렇게
  ![](Pasted%20image%2020231004141451.png)

* 이번에는 온풍기가 있다.
  
  ![](Pasted%20image%2020231004141443.png)

* 이 녀석도 처음에는 따뜻한 바람이 나오다가 점점 세기가 줄어든다. 이렇게
  
  ![](Pasted%20image%2020231004141435.png)
  
  특별히 이 녀석은 1초부터 작동시켰다.

* 이제 이 두 개의 연관관계로 인한 t초에서의 상황을 알고 싶다.

* 여기서 사용되는 연산이 컨볼루션이다.

* 그런데, 상식적으로 생각해 봤을때, 우리는 시작하는 시간이 빠른 순서대로 이 두개가

* 서로 엮임을 알고 있다. 그래서 곱하기전에 둘중 하나를 뒤집어주자.
  
  ![](Pasted%20image%2020231004141427.png)

* 그런데 타우는 뭐지?

* 그걸 이해하기 위해서는 가정이 필요하다.

* 지금 상황에서 내가 난로를 **System** , 온풍기를 **Signal** 로 생각해보면,

* system은 그대로 두고, signal을 멀리서 부터 데리고 온다고 생각해보자.

* 그걸 가능하게 하는 녀석이 타우!

* 제대로 알기 위해서는 t에 대한 함수를 만들어주기 위해 타우라는 더미변수로 적분을 해준다고 이해하면 되겠다.

* 자 이것들을 그림으로 알아보면 이렇게 된다.
  
  ![](Pasted%20image%2020231004141421.png)

* 자 그런데, 앞의 하트시그널 예를 들어보아도, 우리는 한번의 시그널로 인한 결과가 바로 YES로 이어지지는 않는다.

* 결국 시작한 시점부터 누적한 결과가 현재 결과로 다가온다.

* 그래서 겹쳐지는 부분에서 각각의 시간값을 곱한다음 모두 더해준다!
  
  ![](Pasted%20image%2020231004141414.png)

* 이 과정을 식으로 나타낸 것이 컨볼루션이다.

$$ (f * g )(t) = \int\_{-\infty}^\infty f(\tau) g(t - \tau), d\tau \\ (f * g )(t) = \int\_{-\infty}^\infty f(t - \tau) g(\tau), d\tau
$$

# Why Convolution Neural Networks?

* 완전 연결 신경망은 ***변수의 개수가 너무 많다.***
* 사물인식의 관점에서 사물은 시야의 전부를 차지하는 경우가 적다.
* ***부분을 판단***해야한다.
* 즉 ***모든 뉴럴 연결하는 것이 비효율적***일 수 있다.
* 같은 사진에 다른 스타일을 적용하고 싶을때 이걸 가능케한다.

## Edge Detection

![](Pasted%20image%2020231004141406.png)

* 이미지를 구분하는 데 있어 모서리를 구분하자.

![](Pasted%20image%2020231004141357.png)

* 왼쪽은 픽셀의 값이다.
* RGB 채널을 신경쓰지 않을때, 그냥 밝기의 정도만 알려주는 값이라 생각해보자.
* 이 픽셀을 아까 컨볼루션을 이해할 때 사용했던 시스템으로 생각해보자
* 그리고 그 다음에 보이는 3x3 짜리 행렬을 신호라고 생각해보자.
* 이렇게 생각해볼때 이 두개의 행렬을 컨볼루션 한다는 것은,
* 왼쪽행렬의 파란색으로 칠해놓은 것과 같이 행렬을 포개고
* 이를 각각 곱한뒤 모두 더해서 맨 오른쪽 행렬의 **-5** 를 출력하는 과정이다.
* 이 상호작용 이후 나온 결과는 4 x 4 의 행렬로 표현이 되는 것이다.
* 여기서 3x3 행렬을 **Filter** , **Kernel** 이라 한다.

***참고***

* 사실 여기서 Convolution을 한다고 할 때, Signal을 반대방향을 바꿔주는 과정이 있어야한다.

* 그런데 이 뉴럴네트워크를 구성하는데 있어 이것이 핵심적인 역할을 하지 않기 때문에

* 우리는 이 연산자체를 그냥 관습적으로 convolution이라 하고 넘어간다.

* 사실 지금 하는 연산은 정확하게 말하면 Cross-correlation (교차상관) 이라 한다.

* 신호와 시스템에서는 이 연산의 차이가 중요하다..! 다시공부하자.

* 그런데 이 이미지에 대해서 우리가 신경망을 구성하여 Detection 한다는 사실을 안다면,

* 3x3 의 행렬을 이루는 요소는 각각 가중치를 나타내고 있으며,

* 이를 각각의 픽셀값에 곱하고 더하는 행위는 노드의 연결로 인한 값들을 더해주는 것과 같음을 알 수 있다.

* 노드로 생각한다면,
  
  ![https://user-images.githubusercontent.com/37871541/51738815-63704080-20d3-11e9-91c6-d17cc8fc0208.png](https://user-images.githubusercontent.com/37871541/51738815-63704080-20d3-11e9-91c6-d17cc8fc0208.png)
  
  완전연결 신경망과 같이 모든 노드가 연결되는 것이 아니고 위 그림처럼 4개에 대해서만 노드 연결이
  
  이루어 진다고 생각할 수 있다.

# 수직필터

* 자 그런데 위에서 만들어놓은 필터가 세로축을 탐색하는 필터인지 감이 안잡힐 수 있다.
* 다음과 같은 예에서 이 필터를 적용해보자.

![](Pasted%20image%2020231004141347.png)

* 6x6 짜리 밝기를 알려주는 이미지가 있을때 방금전 필터를 합성곱해주게 되면,
* 오른쪽 그림과 같은 결과가 나온다.
* 즉 6x6 이미지에서 필터를 통과하고 난후에 중앙에 흰색값을 반환했다.
* 이미지 가운데 강한 경계선이 있다는 것을 알 수 있다.
* 처음에 0인 값은 경계선을 아직 못만났다는 얘기로 받아들일 수 있다.
* 30과 같이 큰 값은 급격한 경계선을 만났다고 판단할 수 있다.
* 여기서 중요한 사실은 처음에 이미지가 왼쪽이 밝은 상황에서
* 왼쪽이 밝은 수직 필터를 거니 ***밝은 값*** 으로 나왔다는 것이다.
* 즉 수직이미지를 판단할 수 있다.
* 이미지가 커진다면 이 효과가 더 두드러 질 것이다.

### 정리

1. 급격한 경계선을 만나면 값이 커진다.
1. 필터의 이미지의 경향성과 비슷할 경우 밝은 이미지, 반대일 경우 어두워진다.

* 위에서 알아본 같은 필터를 반대 이미지에 걸어보면,
  
  ![](Pasted%20image%2020231004141340.png)

* 이렇게 반대쪽 이미지였다면 ***어두운 값*** 이 출력된다.

* 만약 원래 이미지의 좌우에 따른 밝고 어두움이 중요하지 않다면,

* 출력행렬에 절댓값을 씌우면 된다.

# 수평필터

![](Pasted%20image%2020231004141332.png)

* 자, 이번엔 수평 필터 를 사용했을 때 값을 한번보자.
* 좌우로 이 수평 필터를 걸었을때 수평으로 경계값을 못찾는다면 다 0이다.
* 반대로 수평으로 급격한 경계점을 찾는다면 30을 출력했다.
* 그런데 여기서 필터가 위가 가장밝고 아래로 내려올 수록 어두워지는 경향성을 보인다.
* 이 경향성과 맞는 원래이미지의 6x3 부분의 출력값은
* 출력 행렬의 4x2에 양의 값으로 출력되었다.
* 반대로 출력 이미지의 경향성과 반대인 원래이미지의 오른쪽 부분은
* 크기는 같지만 음의 부호로, 즉 어둡게 표현되었다.
* 이 것은 필터의 경향성을 기준으로,
* **양의 윤곽선** ,**음의 윤곽선** 으로 요약될 수 있다!

# Learning to detect Edges

* 자 앞에서 우리가 알 수 있는 사실은 특적 필터를 사용하면 윤곽선을 얻을 수 있다는 것이다.
* 그런데 우리가 이 윤곽석은 일일히 해보면서 고를 필요는 없다.
* 이 필터를 구성하는 행렬의 요소는 가중치이므로 역전파를 사용해 학습시키면 된다!

![](Pasted%20image%2020231004141326.png)

* 이렇게!
* 여기서 잘 보면 왼쪽의 모든 픽셀들이 다음 층에 연결이 되지 않은 것을 알 수 있다.
* 즉, 다음 층의 노드에는 3개만이 연속해서 연결되어 있는데, 이 3개의 숫자가 곧 필터의 크기를 의미한다.
* 다시 필터이미지로 생각해보면,

![](Pasted%20image%2020231004141318.png)

* 이렇게 가중치를 학습하는 과정이라고 생각하면 된다.
* 즉! 이러한 과정이 연달아서 있게되고,
* 우리는 어떤 이미지를 보았을 때, 그것이 가지는 Edge을 나타낼 수 있는 필터를 찾는게 목표라고 할 수 있다.
* ~~엄밀히 말하면 틀리지만 직관적인 이해를 위해,,~~
* 사실 이 필터는 하위 단계의 속성을 학습한 것이다.
* 즉, 어떤 이미지를 보았을 때 윤곽선을 인지할 수 있게 된것!
* ***신서유기*** 로 비유하면 다음과 같다.

![](Pasted%20image%2020231004141313.png)

![](Pasted%20image%2020231004141526.png)

* 우리가 요정도 해준거다.
* 그림을 보고 저 정도 시야만 인지할 수 있게 만들어준것
* 결국 선인지 아닌지 뭐,, 요정도 가능하다고 생각하면 된다.
